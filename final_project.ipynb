{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG1B39bJk-bs"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to create inverted index\n",
        "def create_inverted_index(file_path):\n",
        "    inverted_index = defaultdict(set)  # Use set instead of list to automatically remove duplicates\n",
        "\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            line_number = int(row['Line Number'])\n",
        "            line_data = row['Data']\n",
        "            tokens = preprocess_text(line_data)\n",
        "            for token in set(tokens):  # Use set to remove duplicate tokens in the same line\n",
        "                inverted_index[token].add(line_number)\n",
        "\n",
        "    return inverted_index\n",
        "\n",
        "# Function to write inverted index to CSV file\n",
        "def write_index_to_csv(index, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Token', 'Line Numbers']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for token, line_numbers in index.items():\n",
        "            writer.writerow({'Token': token, 'Line Numbers': ', '.join(map(str, sorted(line_numbers)))})\n",
        "\n",
        "# Replace 'formatted_news_data.csv' with your actual file path\n",
        "# file_path = 'formatted_news_data.csv'\n",
        "\n",
        "# # Create inverted index\n",
        "# inverted_index = create_inverted_index(file_path)\n",
        "\n",
        "# # Replace 'index.csv' with your desired output file path\n",
        "# output_file = 'index.csv'\n",
        "\n",
        "# # Write inverted index to CSV file\n",
        "# write_index_to_csv(inverted_index, output_file)\n",
        "\n",
        "# print(f\"Inverted index has been written to {output_file}\")\n",
        "def one():\n",
        "    file_path = 'formatted_news_data.csv'\n",
        "\n",
        "    # Create inverted index\n",
        "    inverted_index = create_inverted_index(file_path)\n",
        "\n",
        "    # Replace 'index.csv' with your desired output file path\n",
        "    output_file = 'index.csv'\n",
        "\n",
        "    # Write inverted index to CSV file\n",
        "    write_index_to_csv(inverted_index, output_file)\n",
        "\n",
        "    print(f\"Inverted index has been written to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHDsxECvk-bx"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Function to read inverted index from CSV file\n",
        "def read_inverted_index(file_path):\n",
        "    inverted_index = {}\n",
        "    df = pd.read_csv(file_path)\n",
        "    for index, row in df.iterrows():\n",
        "        token = row['Token']\n",
        "        line_numbers = [int(num) for num in row['Line Numbers'].split(',')]\n",
        "        inverted_index[token] = line_numbers\n",
        "    return inverted_index\n",
        "\n",
        "# Function to preprocess text and get tokens\n",
        "def preprocess_text(text):\n",
        "    stopwords = [\n",
        "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "        \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "        \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
        "        \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
        "        \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
        "        \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
        "        \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
        "        \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
        "        \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
        "        \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
        "    ]\n",
        "\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
        "    tokens = [token for token in tokens if token and token not in stopwords]\n",
        "    return tokens\n",
        "\n",
        "# Function to find line numbers for tokens from the inverted index\n",
        "def find_line_numbers(tokens, inverted_index):\n",
        "    line_numbers = []\n",
        "    for token in tokens:\n",
        "        if token in inverted_index:\n",
        "            line_numbers.extend(inverted_index[token])\n",
        "    return line_numbers\n",
        "\n",
        "# Function to write line numbers list to CSV file\n",
        "def write_line_numbers_to_csv(line_numbers, output_file):\n",
        "    with open(output_file, 'w') as file:\n",
        "        for line_number in line_numbers:\n",
        "            file.write(f\"{line_number}\\n\")\n",
        "\n",
        "# Combined function to perform all tasks\n",
        "def perform_all(user_input):\n",
        "    # Read the inverted index CSV file\n",
        "    file_path = 'index.csv'\n",
        "    output_file = 'formatted_line_numbers.csv'\n",
        "    inverted_index = read_inverted_index(file_path)\n",
        "\n",
        "    # Take input from the user\n",
        "    # user_input = input(\"Enter a sentence: \")\n",
        "\n",
        "    # Preprocess the input and get tokens\n",
        "    tokens = preprocess_text(user_input)\n",
        "\n",
        "    # Find line numbers for tokens from the inverted index\n",
        "    line_numbers = find_line_numbers(tokens, inverted_index)\n",
        "\n",
        "    # Write line numbers list to CSV file\n",
        "    write_line_numbers_to_csv(line_numbers, output_file)\n",
        "\n",
        "    print(f\"Line numbers have been written to {output_file}\")\n",
        "\n",
        "    # Return both the tokens and line numbers\n",
        "    return tokens, line_numbers,user_input\n",
        "\n",
        "# Provide the path to your inverted index CSV file\n",
        "\n",
        "\n",
        "# Replace 'line_numbers.csv' with your desired output file path\n",
        "\n",
        "\n",
        "# Call the combined function to perform all tasks and get the tokens and line numbers\n",
        "# tokens, line_numbers = perform_all(file_path, output_file)\n",
        "\n",
        "# # Now you can use the tokens and line numbers lists as needed in your code\n",
        "# print(\"Returned tokens:\")\n",
        "# print(tokens)\n",
        "# print(\"\\nLine numbers:\")\n",
        "# print(line_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKyJkrQyk-by"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the CSV file containing line numbers and data\n",
        "csv_file_path = 'formatted_news_data.csv'  # Replace 'formatted_news_data.csv' with your CSV file path\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Function to retrieve data corresponding to line numbers\n",
        "def three(line_numbers):\n",
        "    # Initialize an empty string to store the concatenated data\n",
        "    context = \"\"\n",
        "\n",
        "    # Convert line numbers to a set\n",
        "    line_num = set(line_numbers)\n",
        "\n",
        "    # Loop through the set of line numbers and retrieve the corresponding data\n",
        "    for line_number in line_num:\n",
        "        # Check if the line number is within the range of the DataFrame\n",
        "        if line_number in df['Line Number'].values:\n",
        "            # Retrieve the data corresponding to the line number\n",
        "            line_data = df[df['Line Number'] == line_number]['Data'].values[0]\n",
        "            # Concatenate the line data to the context string\n",
        "            context += line_data + '\\n'\n",
        "        else:\n",
        "            print(f\"No data found for line number {line_number}\")\n",
        "\n",
        "    # Return the concatenated data\n",
        "    return context\n",
        "\n",
        "# Define the list of line numbers for which you want to retrieve the data\n",
        "# line_numbers = [1, 2, 3]  # Replace with your desired line numbers\n",
        "\n",
        "# # Call the function to retrieve the data corresponding to the line numbers\n",
        "# context = three(line_numbers)\n",
        "\n",
        "# # Print the concatenated data\n",
        "# print(\"Context:\\n\", context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSUPdscIk-by",
        "outputId": "9b8f35cf-bfbc-4124-8558-fee555faf9cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-23 11:19:22.011929: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-04-23 11:19:22.037042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-23 11:19:22.037074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-23 11:19:22.037758: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-23 11:19:22.042687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-23 11:19:22.698624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "def four(context, question):\n",
        "    # Load model and tokenizer\n",
        "    model_name = \"deepset/roberta-base-squad2\"\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Get predictions\n",
        "    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "    QA_input = {\n",
        "        'question': question,\n",
        "        'context': context\n",
        "    }\n",
        "    res = nlp(QA_input)\n",
        "\n",
        "    return res\n",
        "\n",
        "# Example usage\n",
        "# context = \"\"\"\n",
        "# Pakistani-Canadian columnist Tarek Fatah passed away on 24 April after a prolonged battle with Cancer. Born in Karachi, Pakistan before emigrating to Canada in 1987, Fatah was an award-winning reporter, columnist, and radio and television commentator, both in Canada and abroad. Fatah, who died at 73, was a political activist, a fierce defender of human rights and a staunch opponent of religious fanaticism in any form, nothing scared Tarek Fatah. He also authored several books including, 'Chasing a Mirage: The Tragic Illusion of an Islamic State' and 'The Jew is Not My Enemy: Unveiling the Myths that Fuel Muslim Anti-Smitism.' Mr. Fatah was known for his progressive views on Islam and his fiery stance on Pakistan. He called himself an 'Indian born in Pakistan' and a 'Punjabi born into Islam'. He won awards from organizations such as the Donor Prize, Helen and the Stan Wine Canadian Book Award, and was known for frequent commentary in Canadian, Indian, and international media.\n",
        "# \"\"\"\n",
        "\n",
        "# question = 'Tarek Fatah passed away on?'\n",
        "\n",
        "# result = four(context, question)\n",
        "# print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZoBiaNyk-bz"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "# Function to summarize context chunks\n",
        "def summarize_context(context, max_chunk_length=500, max_summary_length=1000, top_chunks=5):\n",
        "    # Split the context into chunks of maximum length\n",
        "    context_chunks = [context[i:i+max_chunk_length] for i in range(0, len(context), max_chunk_length)]\n",
        "\n",
        "    # Initialize an empty list to store the summaries\n",
        "    summaries = []\n",
        "\n",
        "    # Summarize each chunk separately\n",
        "    for chunk in context_chunks[:top_chunks]:\n",
        "        # Determine the appropriate max_length based on the length of the input chunk\n",
        "        max_length = min(len(chunk) * 2, max_summary_length)\n",
        "        # Summarize the chunk with the dynamically determined max_length\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage:\n",
        "# context = '''Yusuf Raza Gilani, a veteran Pakistan Peoples Party leader and former prime minister, was elected unopposed as chairman of the Senate. Saidal Khan Nasir of the ruling Pakistan Muslim League-Nawaz (PML-N) was elected as the deputy chairman of the Senate. The elections took place amidst protests by Pakistan Tehreek-e-Insaf (PTI) lawmakers, who demanded the postponement of the session until the election of senators from Khyber-Pakhtunkhwa (KP) province.\n",
        "# Kiren Rijiju released the first edition of Indian Constitution in Dogri language PM Narendra Modi launches 'International Big Cats Alliance' on completion of 50 years of Project Tiger Sixteenth Finance Commission to be set up in November 2023 Election Commission grants national party status to AAP, downgrades Sharad Pawar-led Nationalist Congress Party and West Bengal Chief Minister Mamata Banerjee-led Trinamool Congress Solar Energy Corporation of India (SECI) gets ‘Miniratna Category-I’ status PM Modi unveils Rajasthan's first 'Vande Bharat' train Union Minister Nitin Gadkari inaugurates Peerah-Kunfer Tunnel at Ramban FAHD Minister Parshottam Rupala launches “Animal Pandemic Preparedness Initiative (APPI)” under National One Health Mission Madhya Pradesh’s Sehore's “Sharbati wheat” gets GI tag Andhra Pradesh tops State Energy Efficiency Index-2022 Gond painting from Madhya Pradesh receives GI tag 2nd Women20 International Meeting to be held in Jaipur from 13 April India's first semi high-speed regional rail services named 'RAPIDX'\n",
        "# The opposition Congress launched the “Azadi Gaurav Yatra” from the Gandhi Ashram here in Gujarat to commemorate 75 years of Independence and showcase the role played by the party in the freedom struggle and the country’s development post-1947, a kick-off of sorts to the Gujarat election campaign. The yatra will pass through four states before reaching the final destination Raj Ghat in Delhi on June 1. The 1,200-km foot march will cover five districts in Gujarat in the next ten days, Congress leaders said on the occasion.\n",
        "# With a unilateral victory of 58%, Serbia's President Aleksandar Vucic has been elected to a second term in the election. Aleksandar was the joint presidential candidate of the SNS coalition, the Socialist Party of Serbia (SPS), and the Alliance of Vojvodina Hungarians (VMSZ).\n",
        "# '''\n",
        "# ARTICLE = context\n",
        "\n",
        "# summarized_chunks = summarize_context(ARTICLE)\n",
        "# for index, chunk_summary in enumerate(summarized_chunks, 1):\n",
        "#     print(f\"{index}. {chunk_summary}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwt9vu2Nk-b0"
      },
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "from tkinter import ttk\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def analyze_sentiment(news_title):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = analyzer.polarity_scores(news_title)\n",
        "    compound_score = sentiment_score['compound']\n",
        "    if compound_score > 0:\n",
        "        return 'Positive'\n",
        "    elif compound_score < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "def final(user_input):\n",
        "    print(user_input)\n",
        "    one()\n",
        "    tokens, line_numbers ,user_input= perform_all(user_input)\n",
        "    context = three(line_numbers)\n",
        "    result = four(context, user_input)\n",
        "    print(result)\n",
        "\n",
        "\n",
        "    st=''\n",
        "    ARTICLE = context\n",
        "    summarized_chunks = summarize_context(ARTICLE)\n",
        "    for index, chunk_summary in enumerate(summarized_chunks, 1):\n",
        "        st=st+f\"{index}. {chunk_summary}\"\n",
        "        st=st+\"\\n\"\n",
        "        print(f\"{index}. {chunk_summary}\")\n",
        "\n",
        "    return str(result['answer']),st\n",
        "def scrape_news(year_select, month_select, day_select, text_language):\n",
        "    if text_language == \"English\":\n",
        "        text_language = \"1\"\n",
        "    else:\n",
        "        text_language=\"2\"\n",
        "    url = (\n",
        "        \"https://sarkaripariksha.com/gk-and-current-affairs/\"\n",
        "        + year_select\n",
        "        + \"/\"\n",
        "        + month_select\n",
        "        + \"/\"\n",
        "        + str(day_select)\n",
        "        + \"/\"\n",
        "        + text_language\n",
        "        + \"/\"\n",
        "    )\n",
        "    req = requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "    news_data = []\n",
        "    news_list = soup.find_all(\"div\", class_=\"examlist-details-img-box\")\n",
        "\n",
        "    for news_item in news_list:\n",
        "        news_title = news_item.find(\"h2\").find(\"a\").get_text(strip=True)\n",
        "        href_link = news_item.find(\"h2\").find(\"a\")[\"href\"]\n",
        "\n",
        "        # Function to extract the category from the HTML content of a news article\n",
        "        category_set = extract_category(href_link)\n",
        "\n",
        "        # Analyze sentiment\n",
        "        sentiment = analyze_sentiment(news_title)\n",
        "\n",
        "        news_data.append({\"Title\": news_title, \"Category\": category_set, \"Link\": href_link, \"Sentiment\": sentiment})\n",
        "\n",
        "    return pd.DataFrame(news_data)\n",
        "\n",
        "def extract_category(url):\n",
        "    req = requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    category_div = soup.find(\"div\", class_=\"CategoryCurrentAffairsBox\")\n",
        "    return category_div.find(\"span\").text.replace('Category :', '').strip()\n",
        "\n",
        "def load_and_display():\n",
        "    year_select = year_var.get()\n",
        "    month_select = month_var.get()\n",
        "    day_select = day_var.get()\n",
        "    text_language = language_var.get()\n",
        "\n",
        "    try:\n",
        "        news_df = scrape_news(year_select, month_select, day_select, text_language)\n",
        "        news_df.to_csv(\"news_data.csv\", index=False)\n",
        "        category_select = category_var.get()\n",
        "\n",
        "        news_df = pd.read_csv(\"news_data.csv\")\n",
        "\n",
        "        if news_df.empty:\n",
        "            output_text.config(state=tk.NORMAL)\n",
        "            output_text.delete(\"1.0\", tk.END)\n",
        "            output_text.insert(tk.END, \"Data not available for that date.\")\n",
        "            output_text.config(state=tk.DISABLED)\n",
        "            return\n",
        "\n",
        "        if category_select != \"All\":\n",
        "            news_df = news_df[news_df['Category'] == category_select]\n",
        "\n",
        "        context = \"\"\n",
        "        for index, row in news_df.iterrows():\n",
        "            context += f\"News {index+1} : {row['Title']}\\nUrl: {row['Link']}\\n\"\n",
        "\n",
        "\n",
        "        output_text.config(state=tk.NORMAL)\n",
        "        output_text.delete(\"1.0\", tk.END)\n",
        "        output_text.insert(tk.END, context)\n",
        "        output_text.config(state=tk.DISABLED)\n",
        "\n",
        "    except AttributeError as e:\n",
        "        output_text.config(state=tk.NORMAL)\n",
        "        output_text.delete(\"1.0\", tk.END)\n",
        "        output_text.insert(tk.END, \"Data not available for this date\")\n",
        "        output_text.config(state=tk.DISABLED)\n",
        "\n",
        "def process_input():\n",
        "    # Retrieve input from the input text box\n",
        "    user_input = input_textbox.get(\"1.0\", \"end-1c\")\n",
        "\n",
        "    # Process input (here we just reverse the text for demonstration)\n",
        "    processed_output, p = final(str(user_input))\n",
        "\n",
        "    # Clear the output textbox and then update it with the processed output\n",
        "    output_textbox.delete(\"1.0\", \"end\")\n",
        "    output_textbox.insert(\"1.0\", processed_output)\n",
        "\n",
        "    output_textbox_1.delete(\"1.0\", \"end\")\n",
        "    output_textbox_1.insert(\"1.0\", p)\n",
        "\n",
        "# Set up the main window\n",
        "root = tk.Tk()\n",
        "root.title(\"NEWS GENERATOR\")\n",
        "\n",
        "# Set up the frame for layout\n",
        "frame = tk.Frame(root)\n",
        "frame.pack(padx=20, pady=20)\n",
        "\n",
        "# Create the input text box\n",
        "input_label = tk.Label(frame, text=\"Enter your text:\")\n",
        "input_label.pack()\n",
        "input_textbox = tk.Text(frame, height=2, width=40)\n",
        "input_textbox.pack(padx=5, pady=5)\n",
        "\n",
        "# Create the button to trigger processing\n",
        "process_button = tk.Button(frame, text=\"Process\", command=process_input)\n",
        "process_button.pack(pady=10)\n",
        "\n",
        "# Create the output text box\n",
        "output_label = tk.Label(frame, text=\"Processed Answer:\")\n",
        "output_label.pack()\n",
        "output_textbox = tk.Text(frame, height=2, width=40)\n",
        "output_textbox.pack(padx=5, pady=5)\n",
        "\n",
        "output_label_1 = tk.Label(frame, text=\"Processed News:\")\n",
        "output_label_1.pack()\n",
        "output_textbox_1 = tk.Text(frame, height=14, width=100)\n",
        "output_textbox_1.pack(padx=5, pady=5)\n",
        "\n",
        "# Frame for date selection\n",
        "date_frame = ttk.Frame(root)\n",
        "date_frame.pack(pady=10)\n",
        "\n",
        "year_var = tk.StringVar()\n",
        "year_label = ttk.Label(date_frame, text=\"Select year:\")\n",
        "year_label.grid(row=0, column=0, padx=5, pady=5)\n",
        "year_select = ttk.Combobox(date_frame, textvariable=year_var, values=[\"2024\", \"2023\", \"2022\"])\n",
        "year_select.grid(row=0, column=1, padx=5, pady=5)\n",
        "year_select.current(0)\n",
        "\n",
        "month_var = tk.StringVar()\n",
        "month_label = ttk.Label(date_frame, text=\"Select month:\")\n",
        "month_label.grid(row=0, column=2, padx=5, pady=5)\n",
        "month_select = ttk.Combobox(date_frame, textvariable=month_var, values=[\n",
        "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
        "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
        "])\n",
        "month_select.grid(row=0, column=3, padx=5, pady=5)\n",
        "month_select.current(0)\n",
        "\n",
        "day_var = tk.StringVar()\n",
        "day_label = ttk.Label(date_frame, text=\"Select day:\")\n",
        "day_label.grid(row=0, column=4, padx=5, pady=5)\n",
        "day_select = ttk.Combobox(date_frame, textvariable=day_var, values=[str(i) for i in range(1, 32)])\n",
        "day_select.grid(row=0, column=5, padx=5, pady=5)\n",
        "day_select.current(0)\n",
        "\n",
        "# Frame for language selection\n",
        "language_frame = ttk.Frame(root)\n",
        "language_frame.pack(pady=10)\n",
        "\n",
        "language_var = tk.StringVar()\n",
        "language_label = ttk.Label(language_frame, text=\"Select Language:\")\n",
        "language_label.grid(row=0, column=0, padx=5, pady=5)\n",
        "language_select = ttk.Combobox(language_frame, textvariable=language_var, values=[\"English\", \"Hindi\"])\n",
        "language_select.grid(row=0, column=1, padx=5, pady=5)\n",
        "language_select.current(0)\n",
        "\n",
        "# Frame for category selection\n",
        "category_frame = ttk.Frame(root)\n",
        "category_frame.pack(pady=10)\n",
        "\n",
        "category_var = tk.StringVar()\n",
        "category_label = ttk.Label(category_frame, text=\"Select Category:\")\n",
        "category_label.grid(row=0, column=0, padx=5, pady=5)\n",
        "category_select = ttk.Combobox(category_frame, textvariable=category_var, values=[\n",
        "    \"All\", \"Business and economics\", \"Sports\", \"National\", \"International\",\n",
        "    \"Defense\", \"State\", \"Appointment/Resignation\", \"Awards\",\n",
        "    \"Science and Tech\", \"Miscellaneous\"\n",
        "])\n",
        "category_select.grid(row=0, column=1, padx=5, pady=5)\n",
        "category_select.current(0)\n",
        "\n",
        "# Button to load and display news\n",
        "load_button = ttk.Button(root, text=\"Load and Display News\", command=load_and_display)\n",
        "load_button.pack(pady=10)\n",
        "\n",
        "# Output text widget to display news\n",
        "output_text = tk.Text(root, height=20, width=100)\n",
        "output_text.pack(pady=10)\n",
        "output_text.config(state=tk.DISABLED)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX8r__lHk-b1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}